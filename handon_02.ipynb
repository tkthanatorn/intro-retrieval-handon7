{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thanatornkanthala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download([\"stopwords\"])\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess(text):\n",
    "    cleaned_text = text.translate(str.maketrans('', '', '!\"#$%&\\'()*+,.<=>?@[]^`{|}~' + u'\\xa0'))\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    cleaned_text = cleaned_text.translate(str.maketrans(string.whitespace, ' ' * len(string.whitespace), ''))\n",
    "    cleaned_text = ' '.join(['_variable_with_underscore' if '_' in t else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_variable_with_dash' if '-' in t else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_long_variable_name' if len(t) > 15 and t[0] != '#' else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_weburl' if t.startswith('http') and '/' in t else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_number' if re.sub('[\\\\/;:_-]', '', t).isdigit() else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_variable_with_address' if re.match('.*0x[0-9a-f].*', t) else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_name_with_number' if re.match('.*[a-f]*:[0-9]*', t) else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_number_starts_with_one_character' if re.match('[a-f][0-9].*', t) else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_number_starts_with_three_characters' if re.match('[a-f]{3}[0-9].*', t) else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_version' if any(i.isdigit() for i in t) and t.startswith('v') else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_localpath' if ('\\\\' in t or '/' in t) and ':' not in t else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_image_size' if t.endswith('px') else t for t in cleaned_text.split()])\n",
    "    tokenized_text = word_tokenize(cleaned_text)\n",
    "    sw_removed_text = [word for word in tokenized_text if word not in stopword_set]\n",
    "    sw_removed_text = [word for word in sw_removed_text if len(word) > 2]\n",
    "    stemmed_text = ' '.join([stemmer.stem(w) for w in sw_removed_text])\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "import pandas as pd\n",
    "\n",
    "def initialize_pool(stopwords, ps):\n",
    "    global stopword_set\n",
    "    global stemmer\n",
    "    stopword_set = stopwords\n",
    "    stemmer = ps\n",
    "\n",
    "dataset = pd.read_json('data/embold_train.json')\n",
    "dataset.loc[dataset['label'] > 0, 'label'] = -1\n",
    "dataset.loc[dataset['label'] == 0, 'label'] = 1\n",
    "dataset.loc[dataset['label'] == -1, 'label'] = 0\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "pool = Pool(8, initializer=initialize_pool, initargs=(stopwords, ps, ))\n",
    "\n",
    "cleaned_title = pool.map(preprocess, dataset.title)\n",
    "cleaned_body = pool.map(preprocess, dataset.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_texts = pd.DataFrame([cleaned_title, cleaned_body], index=[\"title\", \"body\"]).T\n",
    "y = dataset[\"label\"]\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = train_test_split(data_texts, y, test_size=0.1)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf_vectorizer.fit(cleaned_title + cleaned_body)\n",
    "\n",
    "X_tfidf_fit = tfidf_vectorizer.transform(data_fit[\"title\"])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Number of positive: 48116, number of negative: 59884\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.479068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117994\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2290\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.707882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 118140\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.764795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 118049\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.733795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Total Bins 118016\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.751133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 118007\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445519 -> initscore=-0.218795\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2298\n",
      "[LightGBM] [Info] Start training from score -0.218795\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Number of positive: 48116, number of negative: 59884\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.344921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 118049\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2305\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.352181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 118007\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2298\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.712685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 118016\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2298\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445519 -> initscore=-0.218795\n",
      "[LightGBM] [Info] Start training from score -0.218795\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.719747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 117994\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2290\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.640735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 118140\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Number of positive: 48117, number of negative: 59883\n",
      "[LightGBM] [Info] Number of positive: 48116, number of negative: 59884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.449332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 117994\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2290\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.682592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 118007\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.837674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Total Bins 118140\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.703170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 118049\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2305\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445528 -> initscore=-0.218757\n",
      "[LightGBM] [Info] Start training from score -0.218757\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.501476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 118016\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2298\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445519 -> initscore=-0.218795\n",
      "[LightGBM] [Info] Start training from score -0.218795\n",
      "CV: p:0.7673 r:0.7455 f:0.7484\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "import lightgbm as lgb\n",
    "\n",
    "gbm_model = lgb.LGBMClassifier()\n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "print('CV: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 42029, number of negative: 52471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 105646\n",
      "[LightGBM] [Info] Number of data points in the train set: 94500, number of used features: 2145\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444751 -> initscore=-0.221901\n",
      "[LightGBM] [Info] Start training from score -0.221901\n",
      "test: p:0.7417 r:0.7644 f:0.7445\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, metrics\n",
    "\n",
    "data_fit_train, data_fit_test, y_fit_train, y_fit_test = model_selection.train_test_split(data_fit, y_fit, test_size=0.3)\n",
    "\n",
    "X_tfidf_fit_train = tfidf_vectorizer.transform(data_fit_train['title'])\n",
    "X_tfidf_fit_test = tfidf_vectorizer.transform(data_fit_test['title'])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest['title'])\n",
    "\n",
    "gbm_model.fit(X_tfidf_fit_train, y_fit_train, eval_set=[(X_tfidf_fit_test, y_fit_test)], eval_metric='AUC')\n",
    "\n",
    "precision_test_score = metrics.precision_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "recall_test_score = metrics.recall_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "f1_test_score = metrics.f1_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "\n",
    "print('test: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_test_score, recall_test_score, f1_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanatornkanthala/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-03-09 13:57:36,738] A new study created in memory with name: no-name-af6ae256-e226-46c1-b08c-482e9c1359b6\n",
      "[I 2024-03-09 13:57:46,047] Trial 0 finished with value: 0.7501861476332001 and parameters: {'lambda_l1': 3.5367276724838396e-06, 'lambda_l2': 7.982461664150587e-08, 'num_leaves': 241, 'feature_fraction': 0.8514349516360837, 'bagging_fraction': 0.49104742771900767, 'bagging_freq': 4, 'min_child_samples': 35}. Best is trial 0 with value: 0.7501861476332001.\n",
      "[I 2024-03-09 13:57:51,033] Trial 1 finished with value: 0.7387520921009817 and parameters: {'lambda_l1': 3.90625430058955e-06, 'lambda_l2': 5.810443119133052e-07, 'num_leaves': 125, 'feature_fraction': 0.9301789486092297, 'bagging_fraction': 0.4022211478436718, 'bagging_freq': 6, 'min_child_samples': 68}. Best is trial 0 with value: 0.7501861476332001.\n",
      "[I 2024-03-09 13:57:53,977] Trial 2 finished with value: 0.7508748755658918 and parameters: {'lambda_l1': 1.6881714537023984e-08, 'lambda_l2': 0.3192899446336932, 'num_leaves': 56, 'feature_fraction': 0.6879949532892976, 'bagging_fraction': 0.6603876225404585, 'bagging_freq': 5, 'min_child_samples': 37}. Best is trial 2 with value: 0.7508748755658918.\n",
      "[I 2024-03-09 13:58:01,637] Trial 3 finished with value: 0.741834289422733 and parameters: {'lambda_l1': 0.010943084180861729, 'lambda_l2': 2.1195967925134593, 'num_leaves': 218, 'feature_fraction': 0.7097209124752493, 'bagging_fraction': 0.46349382169458614, 'bagging_freq': 5, 'min_child_samples': 68}. Best is trial 2 with value: 0.7508748755658918.\n",
      "[I 2024-03-09 13:58:06,027] Trial 4 finished with value: 0.73709272652301 and parameters: {'lambda_l1': 9.610935019556441, 'lambda_l2': 0.00020183880332211104, 'num_leaves': 188, 'feature_fraction': 0.8136304043469831, 'bagging_fraction': 0.43516488658562164, 'bagging_freq': 3, 'min_child_samples': 69}. Best is trial 2 with value: 0.7508748755658918.\n",
      "[I 2024-03-09 13:58:10,395] Trial 5 finished with value: 0.747353101502877 and parameters: {'lambda_l1': 3.7990692816611723e-07, 'lambda_l2': 5.618630476073687e-05, 'num_leaves': 99, 'feature_fraction': 0.8127154777455301, 'bagging_fraction': 0.4942408977302147, 'bagging_freq': 2, 'min_child_samples': 63}. Best is trial 2 with value: 0.7508748755658918.\n",
      "[I 2024-03-09 13:58:16,863] Trial 6 finished with value: 0.7428615886564431 and parameters: {'lambda_l1': 0.0005227362549753875, 'lambda_l2': 1.4923149171056625e-08, 'num_leaves': 158, 'feature_fraction': 0.5831837952280764, 'bagging_fraction': 0.42766783849029566, 'bagging_freq': 4, 'min_child_samples': 62}. Best is trial 2 with value: 0.7508748755658918.\n",
      "[I 2024-03-09 13:58:25,884] Trial 7 finished with value: 0.7457861189495195 and parameters: {'lambda_l1': 0.016377022676521327, 'lambda_l2': 2.304105304930408e-07, 'num_leaves': 243, 'feature_fraction': 0.9901031294873573, 'bagging_fraction': 0.43650260759777726, 'bagging_freq': 1, 'min_child_samples': 57}. Best is trial 2 with value: 0.7508748755658918.\n",
      "[I 2024-03-09 13:58:26,973] Trial 8 finished with value: 0.7120412467067532 and parameters: {'lambda_l1': 0.12649503939775128, 'lambda_l2': 0.12404829976597681, 'num_leaves': 7, 'feature_fraction': 0.8993723134120806, 'bagging_fraction': 0.400197452688482, 'bagging_freq': 5, 'min_child_samples': 20}. Best is trial 2 with value: 0.7508748755658918.\n",
      "[I 2024-03-09 13:58:31,379] Trial 9 finished with value: 0.7522452071187462 and parameters: {'lambda_l1': 6.3771444590077875e-06, 'lambda_l2': 1.6626325113214787e-06, 'num_leaves': 108, 'feature_fraction': 0.742004236762802, 'bagging_fraction': 0.990026024621636, 'bagging_freq': 1, 'min_child_samples': 61}. Best is trial 9 with value: 0.7522452071187462.\n",
      "[I 2024-03-09 13:58:35,232] Trial 10 finished with value: 0.7469883966255262 and parameters: {'lambda_l1': 7.697747168256675e-05, 'lambda_l2': 6.008619611862219e-06, 'num_leaves': 85, 'feature_fraction': 0.4251536182529472, 'bagging_fraction': 0.976718729348861, 'bagging_freq': 1, 'min_child_samples': 96}. Best is trial 9 with value: 0.7522452071187462.\n",
      "[I 2024-03-09 13:58:37,537] Trial 11 finished with value: 0.7486951022615165 and parameters: {'lambda_l1': 1.2645616332327141e-08, 'lambda_l2': 0.01608670524604651, 'num_leaves': 39, 'feature_fraction': 0.6501283306178365, 'bagging_fraction': 0.7527038478183902, 'bagging_freq': 7, 'min_child_samples': 41}. Best is trial 9 with value: 0.7522452071187462.\n",
      "[I 2024-03-09 13:58:41,423] Trial 12 finished with value: 0.7562748048421088 and parameters: {'lambda_l1': 1.1305606855166305e-08, 'lambda_l2': 0.0051226747909387076, 'num_leaves': 63, 'feature_fraction': 0.714585400049341, 'bagging_fraction': 0.6869196079789107, 'bagging_freq': 3, 'min_child_samples': 11}. Best is trial 12 with value: 0.7562748048421088.\n",
      "[I 2024-03-09 13:58:50,013] Trial 13 finished with value: 0.7585346434344832 and parameters: {'lambda_l1': 6.208835348750823e-07, 'lambda_l2': 0.0031083731299373866, 'num_leaves': 135, 'feature_fraction': 0.5463338173882272, 'bagging_fraction': 0.9768533057846994, 'bagging_freq': 2, 'min_child_samples': 6}. Best is trial 13 with value: 0.7585346434344832.\n",
      "[I 2024-03-09 13:58:58,628] Trial 14 finished with value: 0.7599556108721802 and parameters: {'lambda_l1': 6.404433610690193e-08, 'lambda_l2': 0.0047683081032282955, 'num_leaves': 147, 'feature_fraction': 0.530358320110102, 'bagging_fraction': 0.8267887532175111, 'bagging_freq': 3, 'min_child_samples': 6}. Best is trial 14 with value: 0.7599556108721802.\n",
      "[I 2024-03-09 13:59:07,263] Trial 15 finished with value: 0.7615918390800236 and parameters: {'lambda_l1': 5.580398622567867e-07, 'lambda_l2': 0.0019337053174094604, 'num_leaves': 159, 'feature_fraction': 0.49924464456092044, 'bagging_fraction': 0.8809665179698055, 'bagging_freq': 3, 'min_child_samples': 9}. Best is trial 15 with value: 0.7615918390800236.\n",
      "[I 2024-03-09 13:59:15,605] Trial 16 finished with value: 0.758955910026724 and parameters: {'lambda_l1': 1.729301688572228e-07, 'lambda_l2': 0.0007455024067046871, 'num_leaves': 173, 'feature_fraction': 0.4226342414021547, 'bagging_fraction': 0.8675331020127977, 'bagging_freq': 3, 'min_child_samples': 23}. Best is trial 15 with value: 0.7615918390800236.\n",
      "[I 2024-03-09 13:59:25,305] Trial 17 finished with value: 0.7597407495961946 and parameters: {'lambda_l1': 9.721958675540207e-05, 'lambda_l2': 3.226935602950892e-05, 'num_leaves': 198, 'feature_fraction': 0.5020836076318569, 'bagging_fraction': 0.8273229083865314, 'bagging_freq': 2, 'min_child_samples': 21}. Best is trial 15 with value: 0.7615918390800236.\n",
      "[I 2024-03-09 13:59:34,014] Trial 18 finished with value: 0.7580604771571571 and parameters: {'lambda_l1': 1.4210734748723e-07, 'lambda_l2': 8.14659864758395, 'num_leaves': 149, 'feature_fraction': 0.515555467942763, 'bagging_fraction': 0.8833137801889193, 'bagging_freq': 4, 'min_child_samples': 6}. Best is trial 15 with value: 0.7615918390800236.\n",
      "[I 2024-03-09 13:59:41,785] Trial 19 finished with value: 0.7400666942184474 and parameters: {'lambda_l1': 1.8500462370549114e-05, 'lambda_l2': 0.05437115134451677, 'num_leaves': 209, 'feature_fraction': 0.6100169520458197, 'bagging_fraction': 0.6069511671076384, 'bagging_freq': 3, 'min_child_samples': 98}. Best is trial 15 with value: 0.7615918390800236.\n",
      "[I 2024-03-09 13:59:49,382] Trial 20 finished with value: 0.749856897862593 and parameters: {'lambda_l1': 0.0021485423353077336, 'lambda_l2': 0.001655858356205595, 'num_leaves': 173, 'feature_fraction': 0.4735074192119881, 'bagging_fraction': 0.7765237214884435, 'bagging_freq': 2, 'min_child_samples': 83}. Best is trial 15 with value: 0.7615918390800236.\n",
      "[I 2024-03-09 13:59:59,444] Trial 21 finished with value: 0.7601310886821774 and parameters: {'lambda_l1': 3.733695327718415e-05, 'lambda_l2': 2.6770209555062233e-05, 'num_leaves': 209, 'feature_fraction': 0.4928531563362083, 'bagging_fraction': 0.8911381912602955, 'bagging_freq': 2, 'min_child_samples': 20}. Best is trial 15 with value: 0.7615918390800236.\n",
      "[I 2024-03-09 14:00:07,484] Trial 22 finished with value: 0.7620306834153262 and parameters: {'lambda_l1': 9.624912795712842e-07, 'lambda_l2': 2.4986488898226195e-05, 'num_leaves': 155, 'feature_fraction': 0.4673566585830895, 'bagging_fraction': 0.9130839071581096, 'bagging_freq': 3, 'min_child_samples': 15}. Best is trial 22 with value: 0.7620306834153262.\n",
      "[I 2024-03-09 14:00:17,863] Trial 23 finished with value: 0.7586659105573856 and parameters: {'lambda_l1': 1.3015593267211567e-06, 'lambda_l2': 1.1444816377309493e-05, 'num_leaves': 224, 'feature_fraction': 0.4606565020711321, 'bagging_fraction': 0.9308071039509207, 'bagging_freq': 2, 'min_child_samples': 29}. Best is trial 22 with value: 0.7620306834153262.\n",
      "[I 2024-03-09 14:00:25,901] Trial 24 finished with value: 0.7555732598051014 and parameters: {'lambda_l1': 2.345378662175509e-05, 'lambda_l2': 0.0002452510569720674, 'num_leaves': 176, 'feature_fraction': 0.40891784873515685, 'bagging_fraction': 0.8998242980776204, 'bagging_freq': 3, 'min_child_samples': 44}. Best is trial 22 with value: 0.7620306834153262.\n",
      "[I 2024-03-09 14:00:37,485] Trial 25 finished with value: 0.7584321398911366 and parameters: {'lambda_l1': 0.000302779264495361, 'lambda_l2': 4.598228186619877e-06, 'num_leaves': 255, 'feature_fraction': 0.5735306652203868, 'bagging_fraction': 0.925800675478778, 'bagging_freq': 4, 'min_child_samples': 17}. Best is trial 22 with value: 0.7620306834153262.\n",
      "[I 2024-03-09 14:00:43,350] Trial 26 finished with value: 0.7598020386586504 and parameters: {'lambda_l1': 1.120953584811675e-06, 'lambda_l2': 8.539959569939895e-05, 'num_leaves': 123, 'feature_fraction': 0.6307090371148815, 'bagging_fraction': 0.7617723881408984, 'bagging_freq': 2, 'min_child_samples': 29}. Best is trial 22 with value: 0.7620306834153262.\n",
      "[I 2024-03-09 14:00:52,180] Trial 27 finished with value: 0.7607012334315623 and parameters: {'lambda_l1': 1.919250114891319e-05, 'lambda_l2': 1.9807285810393258e-05, 'num_leaves': 194, 'feature_fraction': 0.45049868309166846, 'bagging_fraction': 0.8305701129478029, 'bagging_freq': 1, 'min_child_samples': 14}. Best is trial 22 with value: 0.7620306834153262.\n",
      "[I 2024-03-09 14:00:59,689] Trial 28 finished with value: 0.7612634548800974 and parameters: {'lambda_l1': 8.611828447051812e-06, 'lambda_l2': 0.0007357671509187234, 'num_leaves': 155, 'feature_fraction': 0.4579692070351748, 'bagging_fraction': 0.7839358627684422, 'bagging_freq': 1, 'min_child_samples': 13}. Best is trial 22 with value: 0.7620306834153262.\n",
      "[I 2024-03-09 14:01:06,925] Trial 29 finished with value: 0.7513379225890844 and parameters: {'lambda_l1': 3.3541621714749336e-06, 'lambda_l2': 0.000667424377922677, 'num_leaves': 162, 'feature_fraction': 0.5577647546299056, 'bagging_fraction': 0.6186229100490486, 'bagging_freq': 4, 'min_child_samples': 51}. Best is trial 22 with value: 0.7620306834153262.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial):\n",
    "    dtrain = lgb.Dataset(X_tfidf_fit_train, label=y_fit_train)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(param, dtrain)\n",
    "    preds = gbm.predict(X_tfidf_fit_test)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = metrics.roc_auc_score(y_fit_test, pred_labels)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.752389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.567215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.570267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120260\n",
      "[LightGBM] [Info] Total Bins 120280\n",
      "[LightGBM] [Info] Total Bins 120320\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2591\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2610\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2615[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.811740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120294\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2608\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.803597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120342\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2624\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.722962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.862393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 120320\n",
      "[LightGBM] [Info] Total Bins 120342\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2615\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.740165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2624\n",
      "[LightGBM] [Info] Total Bins 120294\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2608\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.754838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120280\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2610\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.374890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 120260\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2591\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Info] Number of positive: 48088, number of negative: 59912\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.586336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120320\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.772321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 120260\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2615\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2591\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.773343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 120294\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2608\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.901781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120280\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2610\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.788595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 120342\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 2624\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445259 -> initscore=-0.219844\n",
      "[LightGBM] [Info] Start training from score -0.219844\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "CV: p:0.7669 r:0.7599 f:0.7620\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] Number of positive: 42056, number of negative: 52444\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107758\n",
      "[LightGBM] [Info] Number of data points in the train set: 94500, number of used features: 2434\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445037 -> initscore=-0.220744\n",
      "[LightGBM] [Info] Start training from score -0.220744\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.702402569232983e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.702402569232983e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6867214598752384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6867214598752384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9096395779799096, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9096395779799096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0003466316295254472, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003466316295254472\n",
      "test: p:0.7588 r:0.7649 f:0.7606\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "gbm_model = lgb.LGBMClassifier(**best_params)\n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "print('CV: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "gbm_model.fit(X_tfidf_fit_train, y_fit_train, eval_set=[(X_tfidf_fit_test, y_fit_test)], eval_metric='AUC')\n",
    "\n",
    "precision_test_score = metrics.precision_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "recall_test_score = metrics.recall_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "f1_test_score = metrics.f1_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "\n",
    "print('test: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_test_score, recall_test_score, f1_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Class Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127454\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] Total Bins 127451\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127454\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127457\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127451\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127454\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] Total Bins 127457\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127451\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "fit: p:0.7211 r:0.7126 f:0.7144\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.496984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214771\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2406\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.590545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.585643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214996\n",
      "[LightGBM] [Info] Total Bins 214895\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2412\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2405\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.598040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214668\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2388\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.599534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214700\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2391\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.537231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214771\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2406\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.550674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214895\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.605569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2405\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092[LightGBM] [Info] Total Bins 214996\n",
      "\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.579805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2412\n",
      "[LightGBM] [Info] Total Bins 214700\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2391\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.578800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214668\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2388\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.553462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214895\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2405\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.692512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214668\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2388\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.560591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214996\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.536913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2412\n",
      "[LightGBM] [Info] Total Bins 214771\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2406\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.588255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214700\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 2391\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "fit: p:0.7600 r:0.7461 f:0.7488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(cleaned_title + cleaned_body)\n",
    "\n",
    "\n",
    "data_fit_train, data_fit_test, y_fit_train, y_fit_test = model_selection.train_test_split(data_fit, y_fit, test_size=0.3)\n",
    "\n",
    "X_tf_fit_train = count_vectorizer.transform(data_fit_train['title'])\n",
    "X_tf_fit_test = count_vectorizer.transform(data_fit_test['title'])\n",
    "X_tf_blindtest = count_vectorizer.transform(data_blindtest['title'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=500, random_state=0)\n",
    "lda.fit(X_tf_fit_train)\n",
    "X_lda_fit = lda.transform(X_tf_fit_train)\n",
    "gbm_model_with_lda = lgb.LGBMClassifier()\n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit_train, cv=5,\n",
    "n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit_train, cv=5,\n",
    "n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit_train, cv=5, n_jobs=-2,\n",
    "scoring='f1_macro').mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "X_tfidf_fit_train = tfidf_vectorizer.transform(data_fit_train['title'])\n",
    "X_fit_with_lda = hstack([X_tfidf_fit_train, X_lda_fit]).tocsr()\n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit_train, cv=5,\n",
    "n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit_train, cv=5,\n",
    "n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit_train, cv=5,\n",
    "n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:11:24,808] A new study created in memory with name: no-name-b02a20b8-9aa6-4381-88f4-5a842e32bf17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16830\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16830\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16830\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16830\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16830\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:12:05,037] Trial 0 finished with value: 0.619762275907522 and parameters: {'n_components': 66, 'learning_decay': 0.7282744881934979}. Best is trial 0 with value: 0.619762275907522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112146\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112147\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112142\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112141\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112141\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:13:52,417] Trial 1 finished with value: 0.7073239891089804 and parameters: {'n_components': 440, 'learning_decay': 0.7900190349207263}. Best is trial 1 with value: 0.7073239891089804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24480\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24480\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24480\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24480\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24480\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:14:37,156] Trial 2 finished with value: 0.6167629083768359 and parameters: {'n_components': 96, 'learning_decay': 0.5863020022513143}. Best is trial 1 with value: 0.7073239891089804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:15:21,194] Trial 3 finished with value: 0.6174270359660184 and parameters: {'n_components': 88, 'learning_decay': 0.9152318322804427}. Best is trial 1 with value: 0.7073239891089804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33150\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33150\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33150\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33150\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33150\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:16:12,673] Trial 4 finished with value: 0.6118712282016608 and parameters: {'n_components': 130, 'learning_decay': 0.8409859515037036}. Best is trial 1 with value: 0.7073239891089804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80580\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 316\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80580\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 316\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80580\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 316\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80580\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 316\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80580\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 316\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:17:33,211] Trial 5 finished with value: 0.6757528346987612 and parameters: {'n_components': 316, 'learning_decay': 0.5338989616004632}. Best is trial 1 with value: 0.7073239891089804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94860\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 372\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94860\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 372\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94860\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 372\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94860\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 372\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94860\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 372\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:19:02,396] Trial 6 finished with value: 0.6822096408995288 and parameters: {'n_components': 372, 'learning_decay': 0.940348183964075}. Best is trial 1 with value: 0.7073239891089804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 78030\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 306\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 78030\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 306\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 78030\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 306\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 78030\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 306\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 78030\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 306\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:20:22,089] Trial 7 finished with value: 0.6581199218230525 and parameters: {'n_components': 306, 'learning_decay': 0.9914480964348377}. Best is trial 1 with value: 0.7073239891089804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 73440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 73440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 73440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 73440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 73440\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:21:37,947] Trial 8 finished with value: 0.6652741758511975 and parameters: {'n_components': 288, 'learning_decay': 0.6843080381462909}. Best is trial 1 with value: 0.7073239891089804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 64770\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 64770\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 64770\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 64770\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 64770\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-09 15:22:49,590] Trial 9 finished with value: 0.6386583956220433 and parameters: {'n_components': 254, 'learning_decay': 0.956545038403525}. Best is trial 1 with value: 0.7073239891089804.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "    n_components = trial.suggest_int('n_components', 2, 500)\n",
    "    learning_decay = trial.suggest_float('learning_decay', 0.5, 1.0)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_components, learning_decay=learning_decay, random_state=0)\n",
    "    lda.fit(X_tf_fit_train)\n",
    "    X_lda_fit = lda.transform(X_tf_fit_train)\n",
    "    \n",
    "    gbm_model_with_lda = lgb.LGBMClassifier()\n",
    "    score = cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit_train, cv=5, scoring='f1_macro', verbose=0).mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Total Bins 127457\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127454\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127454\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127451\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127457\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127454\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33714, number of negative: 41886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127457\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445952 -> initscore=-0.217038\n",
      "[LightGBM] [Info] Start training from score -0.217038\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 33713, number of negative: 41887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127459\n",
      "[LightGBM] [Info] Number of data points in the train set: 75600, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445939 -> initscore=-0.217092\n",
      "[LightGBM] [Info] Start training from score -0.217092\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "CV: p:0.7211 r:0.7126 f:0.7144\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] Number of positive: 42142, number of negative: 52358\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127451\n",
      "[LightGBM] [Info] Number of data points in the train set: 94500, number of used features: 500\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445947 -> initscore=-0.217060\n",
      "[LightGBM] [Info] Start training from score -0.217060\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "[LightGBM] [Warning] Unknown parameter: learning_decay\n",
      "[LightGBM] [Warning] Unknown parameter: n_components\n",
      "test: p:0.7211 r:0.7310 f:0.7231\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "gbm_model = lgb.LGBMClassifier(**best_params)\n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model, X_lda_fit, y_fit_train, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model, X_lda_fit, y_fit_train, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model, X_lda_fit, y_fit_train, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "print('CV: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "\n",
    "X_lda_fit_test = lda.transform(X_tf_fit_test)\n",
    "gbm_model.fit(X_lda_fit, y_fit_train, eval_set=[(X_lda_fit_test, y_fit_test)], eval_metric='AUC')\n",
    "\n",
    "X_lda_blindtest = lda.transform(X_tf_blindtest)\n",
    "\n",
    "precision_test_score = metrics.precision_score(gbm_model.predict(X_lda_blindtest), y_blindtest, average='macro')\n",
    "recall_test_score = metrics.recall_score(gbm_model.predict(X_lda_blindtest), y_blindtest, average='macro')\n",
    "f1_test_score = metrics.f1_score(gbm_model.predict(X_lda_blindtest), y_blindtest, average='macro')\n",
    "\n",
    "print('test: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_test_score, recall_test_score, f1_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn to Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "anime = pd.read_csv(\"./data/anime.csv\")\n",
    "rating = pd.read_csv(\"./data/rating_complete.csv\")\n",
    "\n",
    "anime_features = [\n",
    "    'MAL_ID', 'English name', 'Japanese name', 'Score', 'Genres', 'Popularity',\n",
    "    'Members', 'Favorites', 'Watching', 'Completed', 'On-Hold', 'Dropped',\n",
    "    'Score-1', 'Score-2', 'Score-3', 'Score-4', 'Score-5',\n",
    "    'Score-6', 'Score-7', 'Score-8', 'Score-9', 'Score-10'\n",
    "]\n",
    "anime = anime[anime_features]\n",
    "\n",
    "merged_df = anime.merge(rating, left_on='MAL_ID', right_on='anime_id', how='inner')\n",
    "genre_names = [\n",
    "    'Action', 'Adventure', 'Comedy', 'Drama', 'Sci-Fi',\n",
    "    'Game', 'Space', 'Music', 'Mystery', 'School', 'Fantasy',\n",
    "    'Horror', 'Kids', 'Sports', 'Magic', 'Romance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_to_category(df):\n",
    "    '''Add genre cagegory column'''\n",
    "    d = {name :[] for name in genre_names}\n",
    "\n",
    "    def f(row):\n",
    "        genres = row.Genres.split(',')\n",
    "        for genre in genre_names:\n",
    "            if genre in genres:\n",
    "                d[genre].append(1)\n",
    "            else:\n",
    "                d[genre].append(0)\n",
    "\n",
    "    # create genre category dict\n",
    "    df.apply(f, axis=1)\n",
    "\n",
    "    # add genre category\n",
    "    genre_df = pd.DataFrame(d, columns=genre_names)\n",
    "    df = pd.concat([df, genre_df], axis=1)\n",
    "    return df\n",
    "\n",
    "def make_anime_feature(df):\n",
    "    # convert object to a numeric type, replacing Unknown with nan.\n",
    "    df['Score'] = df['Score'].apply(lambda x: np.nan if x=='Unknown' else float(x))\n",
    "    for i in range(1, 11):\n",
    "        df[f'Score-{i}'] = df[f'Score-{i}'].apply(lambda x: np.nan if x=='Unknown' else float(x))\n",
    "\n",
    "    # add genre ctegory columns\n",
    "    df = genre_to_category(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English name</th>\n",
       "      <th>Japanese name</th>\n",
       "      <th>Score</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Members</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Watching</th>\n",
       "      <th>Completed</th>\n",
       "      <th>On-Hold</th>\n",
       "      <th>Dropped</th>\n",
       "      <th>...</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>School</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Kids</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Magic</th>\n",
       "      <th>Romance</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>rating_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td></td>\n",
       "      <td>8.78</td>\n",
       "      <td>39</td>\n",
       "      <td>1251960</td>\n",
       "      <td>61971</td>\n",
       "      <td>105808</td>\n",
       "      <td>718161</td>\n",
       "      <td>71513</td>\n",
       "      <td>26678</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315</td>\n",
       "      <td>7.603175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td></td>\n",
       "      <td>8.78</td>\n",
       "      <td>39</td>\n",
       "      <td>1251960</td>\n",
       "      <td>61971</td>\n",
       "      <td>105808</td>\n",
       "      <td>718161</td>\n",
       "      <td>71513</td>\n",
       "      <td>26678</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "      <td>7.073955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td></td>\n",
       "      <td>8.78</td>\n",
       "      <td>39</td>\n",
       "      <td>1251960</td>\n",
       "      <td>61971</td>\n",
       "      <td>105808</td>\n",
       "      <td>718161</td>\n",
       "      <td>71513</td>\n",
       "      <td>26678</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>7.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td></td>\n",
       "      <td>8.78</td>\n",
       "      <td>39</td>\n",
       "      <td>1251960</td>\n",
       "      <td>61971</td>\n",
       "      <td>105808</td>\n",
       "      <td>718161</td>\n",
       "      <td>71513</td>\n",
       "      <td>26678</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>679</td>\n",
       "      <td>7.540501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td></td>\n",
       "      <td>8.78</td>\n",
       "      <td>39</td>\n",
       "      <td>1251960</td>\n",
       "      <td>61971</td>\n",
       "      <td>105808</td>\n",
       "      <td>718161</td>\n",
       "      <td>71513</td>\n",
       "      <td>26678</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>8.269841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   English name Japanese name  Score  Popularity  Members  Favorites  \\\n",
       "0  Cowboy Bebop        8.78          39  1251960      61971   \n",
       "1  Cowboy Bebop        8.78          39  1251960      61971   \n",
       "2  Cowboy Bebop        8.78          39  1251960      61971   \n",
       "3  Cowboy Bebop        8.78          39  1251960      61971   \n",
       "4  Cowboy Bebop        8.78          39  1251960      61971   \n",
       "\n",
       "   Watching  Completed  On-Hold  Dropped  ...  Mystery  School  Fantasy  \\\n",
       "0    105808     718161    71513    26678  ...        0       0        0   \n",
       "1    105808     718161    71513    26678  ...        0       0        0   \n",
       "2    105808     718161    71513    26678  ...        0       0        0   \n",
       "3    105808     718161    71513    26678  ...        0       0        0   \n",
       "4    105808     718161    71513    26678  ...        0       0        0   \n",
       "\n",
       "   Horror  Kids  Sports  Magic  Romance  rating_count  rating_mean  \n",
       "0       0     0       0      0        0           315     7.603175  \n",
       "1       0     0       0      0        0           311     7.073955  \n",
       "2       0     0       0      0        0            96     7.656250  \n",
       "3       0     0       0      0        0           679     7.540501  \n",
       "4       0     0       0      0        0            63     8.269841  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_user_feature(df):\n",
    "    # add user feature\n",
    "    df['rating_count'] = df.groupby('user_id')['anime_id'].transform('count')\n",
    "    df['rating_mean'] = df.groupby('user_id')['rating'].transform('mean')\n",
    "    return df\n",
    "\n",
    "def preprocess(merged_df):\n",
    "    merged_df = make_anime_feature(merged_df)\n",
    "    merged_df = make_user_feature(merged_df)\n",
    "    return merged_df\n",
    "\n",
    "merged_df = preprocess(merged_df)\n",
    "merged_df = merged_df.drop(['MAL_ID', 'Genres'], axis=1)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit, blindtest = train_test_split(merged_df, test_size=0.2, random_state=0)\n",
    "fit_train, fit_test = train_test_split(fit, test_size=0.3, random_state=0)\n",
    "\n",
    "features = ['Score', 'Popularity','Members',\n",
    "            'Favorites','Watching','Completed','On-Hold','Dropped',\n",
    "            'Score-1','Score-2','Score-3','Score-4','Score-5',\n",
    "            'Score-6','Score-7','Score-8','Score-9','Score-10',\n",
    "            'rating_count','rating_mean']\n",
    "features += genre_names\n",
    "user_col = 'user_id'\n",
    "item_col = 'anime_id'\n",
    "target_col = 'rating'\n",
    "\n",
    "fit_train = fit_train.sort_values('user_id').reset_index(drop=True)\n",
    "fit_test = fit_test.sort_values('user_id').reset_index(drop=True)\n",
    "blindtest = blindtest.sort_values('user_id').reset_index(drop=True)\n",
    "\n",
    "# Model query data\n",
    "fit_train_query = fit_train[user_col].value_counts().sort_index()\n",
    "fit_test_query = fit_test[user_col].value_counts().sort_index()\n",
    "blindtest_query = blindtest[user_col].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5088\n",
      "[LightGBM] [Info] Number of data points in the train set: 32274635, number of used features: 36\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[733]\tvalid_0's ndcg@1: 0.713589\tvalid_0's ndcg@3: 0.723082\tvalid_0's ndcg@5: 0.739184\tvalid_0's ndcg@10: 0.773832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.08830729, -1.9089911 , -2.40144488, -0.84965541,  0.04302394,\n",
       "        0.20422849, -0.78542115,  0.25960152,  0.48935606, -0.48269437])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMRanker(n_estimators=1000, random_state=0)\n",
    "model.fit(\n",
    "    fit_train[features],\n",
    "    fit_train[target_col],\n",
    "    group=fit_train_query,\n",
    "    eval_set=[(fit_test[features], fit_test[target_col])],\n",
    "    eval_group=[list(fit_test_query)],\n",
    "    eval_at=[1, 3, 5, 10],  # calc validation ndcg@1,3,5,10\n",
    "    # early_stopping_rounds=100,\n",
    "    # verbose=10\n",
    "    callbacks=[\n",
    "        lgb.callback.early_stopping(100),\n",
    "    ],\n",
    ")\n",
    "\n",
    "#%%\n",
    "model.predict(blindtest.iloc[:10][features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Feature Importance')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=(10, 7))\n",
    "df_plt = pd.DataFrame({'feature_name': features, 'feature_importance': model.feature_importances_})\n",
    "df_plt.sort_values('feature_importance', ascending=False, inplace=True)\n",
    "sns.barplot(x=\"feature_importance\", y=\"feature_name\", data=df_plt)\n",
    "plt.title('Feature Importance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(user_df, top_k, anime, rating):\n",
    "    user_anime_df = anime.merge(user_df, left_on='MAL_ID', right_on='anime_id')\n",
    "    user_anime_df = make_anime_feature(user_anime_df)\n",
    "\n",
    "    excludes_genres = list(np.array(genre_names)[np.nonzero([user_anime_df[genre_names].sum(axis=0) <= 1])[1]])\n",
    "\n",
    "    pred_df = make_anime_feature(anime.copy())\n",
    "    pred_df = pred_df.loc[pred_df[excludes_genres].sum(axis=1) == 0]\n",
    "\n",
    "    for col in user_df.columns:\n",
    "        if col in features:\n",
    "            pred_df[col] = user_df[col].values[0]\n",
    "\n",
    "    preds = model.predict(pred_df[features])\n",
    "\n",
    "    topk_idx = np.argsort(preds)[::-1][:top_k]\n",
    "\n",
    "    recommend_df = pred_df.iloc[topk_idx].reset_index(drop=True)\n",
    "\n",
    "    # Check recommendation\n",
    "    print('---------- Recommend ----------')\n",
    "    for i, row in recommend_df.iterrows():\n",
    "        print(f'{i+1}: {row[\"Japanese name\"]}:{row[\"English name\"]}')\n",
    "\n",
    "    print('---------- Rated ----------')\n",
    "    user_df = user_df.merge(anime, left_on='anime_id', right_on='MAL_ID', how='inner')\n",
    "    for i, row in user_df.sort_values('rating', ascending=False).iterrows():\n",
    "        print(f'rating:{row[\"rating\"]}: {row[\"Japanese name\"]}:{row[\"English name\"]}')\n",
    "\n",
    "    return recommend_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>430</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1004</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3010</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2762</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  anime_id  rating\n",
       "0        0       430       9\n",
       "1        0      1004       5\n",
       "2        0      3010       7\n",
       "3        0       570       7\n",
       "4        0      2762       9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Recommend ----------\n",
      "1:  THE FINAL:Unknown\n",
      "2: :Legend of the Galactic Heroes\n",
      "3:  THE MOVIE 3:Unknown\n",
      "4: S:Unknown\n",
      "5:  :Unknown\n",
      "6: :Rocky Joe 2\n",
      "7: !! :Unknown\n",
      "8: :Gintama Season 4\n",
      "9: :Ijiranaide, Nagatoro San\n",
      "10:  The Final Season:Attack on Titan Final Season\n",
      "---------- Rated ----------\n",
      "rating:10: :Ghost Hunt\n",
      "rating:10: :The Girl Who Leapt Through Time\n",
      "rating:10:  :My Neighbors the Yamadas\n",
      "rating:10: :Grave of the Fireflies\n",
      "rating:9:   :Fullmetal Alchemist:The Movie - Conqueror of Shamballa\n",
      "rating:9: ONE PIECE:Unknown\n",
      "rating:9: Fate/stay night:Fate/stay night\n",
      "rating:9: :Fullmetal Alchemist\n",
      "rating:9: :Unknown\n",
      "rating:9: :Unknown\n",
      "rating:8: SAMURAI DEEPER KYO:Samurai Deeper Kyo\n",
      "rating:8: :Lovely Complex\n",
      "rating:8: :Howl's Moving Castle\n",
      "rating:8: :Princess Mononoke\n",
      "rating:8: :Spirited Away\n",
      "rating:7: :Tales from Earthsea\n",
      "rating:7: :The Magnificent Zorro\n",
      "rating:7:  :One Piece:Defeat the Pirate Ganzack!\n",
      "rating:7: :The Great Adventures of Robin Hood\n",
      "rating:7: :Yu-Gi-Oh!:The Movie\n",
      "rating:7:  JIN-ROH:Jin-Roh:The Wolf Brigade\n",
      "rating:7: :Elemental Gelade\n",
      "rating:7: :A Little Princess Sara\n",
      "rating:7:  :Lunar Legend Tsukihime\n",
      "rating:6: On Your Mark CHAGE & ASKA:On Your Mark\n",
      "rating:6: :Hidden Leaf Village Grand Sports Festival\n",
      "rating:6: :Unknown\n",
      "rating:6: :The Place Promised in Our Early Days\n",
      "rating:6: LEGEND OF DUO:Unknown\n",
      "rating:6: :Black Cat\n",
      "rating:6: :5 Centimeters Per Second\n",
      "rating:6: :Bush Baby, Little Angel of the Great Plains\n",
      "rating:5: :She and Her Cat:Their Standing Points\n",
      "rating:5: ULTIMATE FIGHT:Tenjho Tenge:The Ultimate Fight\n",
      "rating:4: :Tenjho Tenge\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAL_ID</th>\n",
       "      <th>English name</th>\n",
       "      <th>Japanese name</th>\n",
       "      <th>Score</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Members</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Watching</th>\n",
       "      <th>Completed</th>\n",
       "      <th>...</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>School</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Kids</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Magic</th>\n",
       "      <th>Romance</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>rating_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39486</td>\n",
       "      <td>Unknown</td>\n",
       "      <td> THE FINAL</td>\n",
       "      <td>8.88</td>\n",
       "      <td>Action, Sci-Fi, Comedy, Historical, Parody, Dr...</td>\n",
       "      <td>2987</td>\n",
       "      <td>29811</td>\n",
       "      <td>902</td>\n",
       "      <td>1465</td>\n",
       "      <td>2640</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>820</td>\n",
       "      <td>Legend of the Galactic Heroes</td>\n",
       "      <td></td>\n",
       "      <td>9.07</td>\n",
       "      <td>Military, Sci-Fi, Space, Drama</td>\n",
       "      <td>633</td>\n",
       "      <td>230168</td>\n",
       "      <td>13834</td>\n",
       "      <td>25968</td>\n",
       "      <td>52583</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44200</td>\n",
       "      <td>Unknown</td>\n",
       "      <td> THE MOVIE 3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Action, Comedy, Super Power, School, Shounen</td>\n",
       "      <td>2501</td>\n",
       "      <td>41961</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39247</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slice of Life, Comedy, Fantasy</td>\n",
       "      <td>1346</td>\n",
       "      <td>106489</td>\n",
       "      <td>627</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47778</td>\n",
       "      <td>Unknown</td>\n",
       "      <td> </td>\n",
       "      <td>NaN</td>\n",
       "      <td>Action, Historical, Demons, Supernatural, Shounen</td>\n",
       "      <td>2078</td>\n",
       "      <td>59440</td>\n",
       "      <td>547</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2921</td>\n",
       "      <td>Rocky Joe 2</td>\n",
       "      <td></td>\n",
       "      <td>8.67</td>\n",
       "      <td>Action, Drama, Shounen, Slice of Life, Sports</td>\n",
       "      <td>2966</td>\n",
       "      <td>30192</td>\n",
       "      <td>1546</td>\n",
       "      <td>1378</td>\n",
       "      <td>14113</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40089</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>!! </td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>12638</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28977</td>\n",
       "      <td>Gintama Season 4</td>\n",
       "      <td></td>\n",
       "      <td>9.10</td>\n",
       "      <td>Action, Comedy, Historical, Parody, Samurai, S...</td>\n",
       "      <td>329</td>\n",
       "      <td>404121</td>\n",
       "      <td>11868</td>\n",
       "      <td>48011</td>\n",
       "      <td>167130</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42361</td>\n",
       "      <td>Ijiranaide, Nagatoro San</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slice of Life, Comedy, Romance</td>\n",
       "      <td>1784</td>\n",
       "      <td>73950</td>\n",
       "      <td>756</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40028</td>\n",
       "      <td>Attack on Titan Final Season</td>\n",
       "      <td> The Final Season</td>\n",
       "      <td>9.17</td>\n",
       "      <td>Action, Military, Mystery, Super Power, Drama,...</td>\n",
       "      <td>119</td>\n",
       "      <td>733260</td>\n",
       "      <td>44862</td>\n",
       "      <td>566239</td>\n",
       "      <td>553</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MAL_ID                   English name            Japanese name  Score  \\\n",
       "0   39486                        Unknown              THE FINAL   8.88   \n",
       "1     820  Legend of the Galactic Heroes                      9.07   \n",
       "2   44200                        Unknown   THE MOVIE 3    NaN   \n",
       "3   39247                        Unknown            S    NaN   \n",
       "4   47778                        Unknown                      NaN   \n",
       "5    2921                    Rocky Joe 2                    8.67   \n",
       "6   40089                        Unknown            !!     NaN   \n",
       "7   28977               Gintama Season 4                         9.10   \n",
       "8   42361       Ijiranaide, Nagatoro San                  NaN   \n",
       "9   40028   Attack on Titan Final Season    The Final Season   9.17   \n",
       "\n",
       "                                              Genres  Popularity  Members  \\\n",
       "0  Action, Sci-Fi, Comedy, Historical, Parody, Dr...        2987    29811   \n",
       "1                     Military, Sci-Fi, Space, Drama         633   230168   \n",
       "2       Action, Comedy, Super Power, School, Shounen        2501    41961   \n",
       "3                     Slice of Life, Comedy, Fantasy        1346   106489   \n",
       "4  Action, Historical, Demons, Supernatural, Shounen        2078    59440   \n",
       "5      Action, Drama, Shounen, Slice of Life, Sports        2966    30192   \n",
       "6                                            Unknown       12638      412   \n",
       "7  Action, Comedy, Historical, Parody, Samurai, S...         329   404121   \n",
       "8                     Slice of Life, Comedy, Romance        1784    73950   \n",
       "9  Action, Military, Mystery, Super Power, Drama,...         119   733260   \n",
       "\n",
       "   Favorites  Watching  Completed  ...  Mystery  School  Fantasy  Horror  \\\n",
       "0        902      1465       2640  ...        0       0        0       0   \n",
       "1      13834     25968      52583  ...        0       0        0       0   \n",
       "2        195         0          3  ...        0       0        0       0   \n",
       "3        627         4          1  ...        0       0        0       0   \n",
       "4        547        17          3  ...        0       0        0       0   \n",
       "5       1546      1378      14113  ...        0       0        0       0   \n",
       "6          0       198          0  ...        0       0        0       0   \n",
       "7      11868     48011     167130  ...        0       0        0       0   \n",
       "8        756         9          2  ...        0       0        0       0   \n",
       "9      44862    566239        553  ...        0       0        0       0   \n",
       "\n",
       "   Kids  Sports  Magic  Romance  rating_count  rating_mean  \n",
       "0     0       0      0        0            35          7.4  \n",
       "1     0       0      0        0            35          7.4  \n",
       "2     0       0      0        0            35          7.4  \n",
       "3     0       0      0        0            35          7.4  \n",
       "4     0       0      0        0            35          7.4  \n",
       "5     0       0      0        0            35          7.4  \n",
       "6     0       0      0        0            35          7.4  \n",
       "7     0       0      0        0            35          7.4  \n",
       "8     0       0      0        0            35          7.4  \n",
       "9     0       0      0        0            35          7.4  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 0\n",
    "user_df = rating.copy().loc[rating['user_id'] == user_id]\n",
    "user_df = make_user_feature(user_df)\n",
    "predict(user_df, 10, anime, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
